{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMGcfXdQLUT9",
        "outputId": "eac0a3db-170d-42c5-cbaf-cf6198abfa35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks')\n",
        "\n",
        "!pip install keras-tuner\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "4sG2D4TEkj8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4003340d-73fc-4a50-e7bc-d8a124028992"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.9/dist-packages (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (3.20.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.53.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.32.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (67.6.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.7)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.22.4)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (16.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (0.0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.17.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (6.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.47)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.2 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "createTriplet = importlib.import_module('createTriplet')"
      ],
      "metadata": {
        "id": "CkRp6CGH6s0H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import gc\n",
        "import kerastuner as kt\n",
        "import cv2\n",
        "import createTriplet\n",
        "from createTriplet import augment_images, create_triplets\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Import the necessary modules for pre-trained model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# Triplet loss function\n",
        "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
        "    embedding_size = y_pred.shape[-1] // 3\n",
        "    anchor, positive, negative = y_pred[:, :embedding_size], y_pred[:, embedding_size:2*embedding_size], y_pred[:, 2*embedding_size:]\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "    loss = tf.maximum(pos_dist - neg_dist + alpha, 0.0)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Create embedding model with EfficientNetB0\n",
        "def create_embedding_model(input_shape, l2_strength=0.01):\n",
        "    base_model = EfficientNetB0(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(l2_strength))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(l2_strength))(x)\n",
        "    return Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Load images\n",
        "path_to_directory = \"/content/drive/MyDrive/Datasets/One_images\"\n",
        "image_paths = [os.path.join(path_to_directory, file) for file in os.listdir(path_to_directory) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Create a list of images and their corresponding class labels\n",
        "images = []\n",
        "labels = []\n",
        "num_augmented_images = 6\n",
        "for image_path in image_paths:\n",
        "    augmented_images = augment_images(image_path, num_augmented_images=num_augmented_images)\n",
        "    class_name = os.path.basename(image_path).split('_')[0]  # Extract class name from the image path\n",
        "    images.extend(augmented_images)\n",
        "    labels.extend([class_name] * len(augmented_images))\n",
        "\n",
        "# Load and preprocess image\n",
        "def load_image(image_data, target_size=(224, 224)):\n",
        "    if isinstance(image_data, str):\n",
        "        img = img_to_array(load_img(image_data, target_size=target_size))\n",
        "    elif isinstance(image_data, np.ndarray):\n",
        "        img = cv2.resize(image_data, target_size)\n",
        "    else:\n",
        "        raise TypeError(f\"image_data should be a string or a numpy array, not {type(image_data)}\")\n",
        "    \n",
        "    return preprocess_input(img)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, train_size=0.8, stratify=labels)\n",
        "\n",
        "# Free up memory\n",
        "del images\n",
        "del labels\n",
        "gc.collect()\n",
        "\n",
        "train_anchor_images, train_positive_images, train_negative_images = create_triplets(train_images, train_labels)\n",
        "val_anchor_images, val_positive_images, val_negative_images = create_triplets(val_images, val_labels)\n",
        "\n",
        "# Free up memory\n",
        "del train_images\n",
        "del val_images\n",
        "del train_labels\n",
        "del val_labels\n",
        "gc.collect()\n",
        "\n",
        "loaded_train_anchor_images = np.array([load_image(img) for img in train_anchor_images])\n",
        "loaded_train_positive_images = np.array([load_image(img) for img in train_positive_images])\n",
        "loaded_train_negative_images = np.array([load_image(img) for img in train_negative_images])\n",
        "\n",
        "# Load the images for the validation triplets\n",
        "loaded_val_anchor_images = np.array([load_image(img) for img in val_anchor_images])\n",
        "loaded_val_positive_images = np.array([load_image(img) for img in val_positive_images])\n",
        "loaded_val_negative_images = np.array([load_image(img) for img in val_negative_images])\n",
        "\n",
        "num_train_triplets = len(train_anchor_images)\n",
        "num_val_triplets = len(val_anchor_images)\n",
        "\n",
        "# Free up memory\n",
        "del train_anchor_images\n",
        "del train_positive_images\n",
        "del train_negative_images\n",
        "del val_anchor_images\n",
        "del val_positive_images\n",
        "del val_negative_images\n",
        "gc.collect()\n",
        "\n",
        "def model_builder(trial):\n",
        "    # Create the base network for embeddings\n",
        "    input_shape = (224, 224, 3)\n",
        "\n",
        "    # Hyperparameters\n",
        "    l2_strength = trial.suggest_loguniform(\"l2_strength\", 1e-6, 1e-2)\n",
        "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n",
        "    dense_units = trial.suggest_categorical(\"dense_units\", [128, 256, 512])\n",
        "\n",
        "    embedding_model = create_embedding_model(input_shape, l2_strength=l2_strength)\n",
        "\n",
        "    # Add dropout layers to the embedding model\n",
        "    x = embedding_model.output\n",
        "    for i in range(trial.suggest_int(\"num_dropout_layers\", 1, 3)):\n",
        "        x = Dropout(rate=dropout_rate, name=f\"dropout_layer_{i+1}\")(x)\n",
        "    x = Dense(dense_units, activation=\"relu\", kernel_regularizer=regularizers.l2(l2_strength), name=\"dense_layer\")(x)\n",
        "    modified_embedding_model = Model(inputs=embedding_model.input, outputs=x)\n",
        "\n",
        "    # Create the Siamese network with triplet loss\n",
        "    anchor_input = Input(shape=input_shape, name=\"anchor_input\")\n",
        "    positive_input = Input(shape=input_shape, name=\"positive_input\")\n",
        "    negative_input = Input(shape=input_shape, name=\"negative_input\")\n",
        "\n",
        "    anchor_embedding = modified_embedding_model(anchor_input)\n",
        "    positive_embedding = modified_embedding_model(positive_input)\n",
        "    negative_embedding = modified_embedding_model(negative_input)\n",
        "\n",
        "    outputs = tf.keras.layers.concatenate([anchor_embedding, positive_embedding, negative_embedding], axis=-1)\n",
        "    siamese_triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=outputs)\n",
        "\n",
        "    # Learning Rate\n",
        "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the model\n",
        "    siamese_triplet_model.compile(optimizer=Adam(learning_rate), loss=triplet_loss)\n",
        "\n",
        "    return siamese_triplet_model\n",
        "\n",
        "def objective(trial):\n",
        "    model = model_builder(trial)\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    model.fit(\n",
        "        [loaded_train_anchor_images, loaded_train_positive_images, loaded_train_negative_images],\n",
        "        np.zeros(num_train_triplets),\n",
        "        validation_data=([loaded_val_anchor_images, loaded_val_positive_images, loaded_val_negative_images], np.zeros(num_val_triplets)),\n",
        "        batch_size=32,\n",
        "        epochs=20,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    loss = model.evaluate([loaded_val_anchor_images, loaded_val_positive_images, loaded_val_negative_images], np.zeros(num_val_triplets), verbose=0)\n",
        "    return loss\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hyperparameters = study.best_trial.params\n",
        "print(\"Best hyperparameters: \", best_hyperparameters)\n",
        "\n",
        "# Build the best model with the best_hyperparameters and train it\n",
        "best_model = model_builder(optuna.trial.FixedTrial(best_hyperparameters))\n",
        "\n",
        "\n",
        "\n",
        "# Train the best model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "best_model.fit(\n",
        "    [loaded_train_anchor_images, loaded_train_positive_images, loaded_train_negative_images],\n",
        "    np.zeros(num_train_triplets),\n",
        "    validation_data=([loaded_val_anchor_images, loaded_val_positive_images, loaded_val_negative_images], np.zeros(num_val_triplets)),\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]  # Pass the EarlyStopping callback to the fit method\n",
        ")\n",
        "\n",
        "# Get the best embedding model from the best Siamese network\n",
        "best_embedding_model = Model(inputs=best_model.get_layer(index=2).get_input_at(0), outputs=best_model.get_layer(index=2).get_output_at(0))\n",
        "\n",
        "# Save the best embedding model\n",
        "tf.saved_model.save(best_model, \"best_siamese_model\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzrO2pwhEVdf",
        "outputId": "eed310b4-d6e3-4222-d2e2-eba97065c0ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:40:40,762]\u001b[0m A new study created in memory with name: no-name-e04119e6-d717-4115-9296-3a44a70e5387\u001b[0m\n",
            "<ipython-input-11-1d8287abab42>:115: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  l2_strength = trial.suggest_loguniform(\"l2_strength\", 1e-6, 1e-2)\n",
            "<ipython-input-11-1d8287abab42>:116: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 71s 429ms/step - loss: 39.6053 - val_loss: 10.4080\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 257ms/step - loss: 16.4149 - val_loss: 10.4071\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 11.8524 - val_loss: 10.3844\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 10.6795 - val_loss: 10.3526\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 10.4429 - val_loss: 10.3161\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 10.3562 - val_loss: 10.2748\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 8s 250ms/step - loss: 10.3106 - val_loss: 1721592192.0000\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 8s 252ms/step - loss: 10.2373 - val_loss: 1879143936.0000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 10.1831 - val_loss: 512443.1875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:43:05,232]\u001b[0m Trial 0 finished with value: 10.274825096130371 and parameters: {'l2_strength': 0.007306300598138226, 'dropout_rate': 0.17107117079162043, 'dense_units': 256, 'num_dropout_layers': 3, 'learning_rate': 0.0001}. Best is trial 0 with value: 10.274825096130371.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 74s 542ms/step - loss: 194.7104 - val_loss: 2.6410\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 250ms/step - loss: 45.2518 - val_loss: 2.6448\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 9.3430 - val_loss: 2.6444\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 258ms/step - loss: 4.1735 - val_loss: 2.6433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:44:51,966]\u001b[0m Trial 1 finished with value: 2.641005039215088 and parameters: {'l2_strength': 0.001704025348463876, 'dropout_rate': 0.47723534023381864, 'dense_units': 512, 'num_dropout_layers': 3, 'learning_rate': 0.0001}. Best is trial 1 with value: 2.641005039215088.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 69s 426ms/step - loss: 271.1966 - val_loss: 15.7473\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 249ms/step - loss: 22.7819 - val_loss: 16.8253\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 16.8228 - val_loss: 16.7339\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 16.6509 - val_loss: 16.5515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:46:33,688]\u001b[0m Trial 2 finished with value: 15.747269630432129 and parameters: {'l2_strength': 0.00635774799639409, 'dropout_rate': 0.21171102470464898, 'dense_units': 512, 'num_dropout_layers': 3, 'learning_rate': 0.001}. Best is trial 1 with value: 2.641005039215088.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 70s 453ms/step - loss: 14016.4932 - val_loss: 24185.0957\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 165.5417 - val_loss: 12.7789\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 253ms/step - loss: 21.3782 - val_loss: 4.1634\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 10.8622 - val_loss: 4.1619\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 7.0494 - val_loss: 4.1611\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 5.8883 - val_loss: 4.1599\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 5.4308 - val_loss: 4.1589\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 5.3633 - val_loss: 4.1576\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 5.0514 - val_loss: 4.1565\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 5.1409 - val_loss: 4.1550\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 5.6366 - val_loss: 4.1540\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 4.6202 - val_loss: 4.1525\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 4.5135 - val_loss: 4.1509\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 4.6546 - val_loss: 4.1492\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 4.3285 - val_loss: 4.1474\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 4.2914 - val_loss: 4.1456\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 8s 253ms/step - loss: 4.2735 - val_loss: 4.1437\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 4.2214 - val_loss: 4.1417\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 4.4294 - val_loss: 4.1397\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 8s 254ms/step - loss: 4.2693 - val_loss: 4.1382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:50:26,829]\u001b[0m Trial 3 finished with value: 4.138205528259277 and parameters: {'l2_strength': 3.673750863784056e-05, 'dropout_rate': 0.13429297384147398, 'dense_units': 256, 'num_dropout_layers': 3, 'learning_rate': 0.01}. Best is trial 1 with value: 2.641005039215088.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 75s 435ms/step - loss: 38582.9922 - val_loss: 1.8028\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 257ms/step - loss: 217.8607 - val_loss: 1.1118\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 10.8777 - val_loss: 1.1162\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 253ms/step - loss: 3.0166 - val_loss: 1.1164\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 2.0570 - val_loss: 1.1164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-03-31 15:52:23,120]\u001b[0m Trial 4 finished with value: 1.111788272857666 and parameters: {'l2_strength': 8.67906541302328e-06, 'dropout_rate': 0.3060815370347101, 'dense_units': 512, 'num_dropout_layers': 3, 'learning_rate': 0.01}. Best is trial 4 with value: 1.111788272857666.\u001b[0m\n",
            "<ipython-input-11-1d8287abab42>:115: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FixedTrial.suggest_float` instead.\n",
            "  l2_strength = trial.suggest_loguniform(\"l2_strength\", 1e-6, 1e-2)\n",
            "<ipython-input-11-1d8287abab42>:116: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FixedTrial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'l2_strength': 8.67906541302328e-06, 'dropout_rate': 0.3060815370347101, 'dense_units': 512, 'num_dropout_layers': 3, 'learning_rate': 0.01}\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 70s 454ms/step - loss: 65241.8242 - val_loss: 2768.6770\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 1216.2914 - val_loss: 1.1394\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 94.4799 - val_loss: 1.1457\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 8s 251ms/step - loss: 11.9003 - val_loss: 1.1460\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 8s 255ms/step - loss: 5.9073 - val_loss: 1.1461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}